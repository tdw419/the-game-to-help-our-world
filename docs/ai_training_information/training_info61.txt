**Welcome to the AI↔AI Collaboration Environment!**
This guide will walk you through everything you need to get started with the single‐file `kernal.py`, which bundles both the “hub” (broker + dashboard) and the “trainer” (AI node) into one script. By the end, you’ll know how to launch the hub, connect multiple trainers, observe live statistics, and run automated scenarios.

---

## 1. Prerequisites

1. **Clone or Download the ColorOS Repository**
   Ensure you have a local copy of the `ColorOS` folder, and that `kernal.py` is placed directly under it. For example:

   ```
   ColorOS/
   └─ kernal.py
   ```

2. **Python 3.8+ Installed**
   Verify you have Python 3.8 or later:

   ```bash
   python3 --version
   ```

3. **Create a Virtual Environment (Recommended)**
   We recommend isolating dependencies in a virtual environment:

   ```bash
   cd /path/to/ColorOS
   python3 -m venv venv
   source venv/bin/activate
   ```

4. **Install Required Packages**
   The only external dependency is Flask (optional: Matplotlib for harness charts).

   ```bash
   pip install flask matplotlib
   ```

---

## 2. Overview of Components

* **`kernal.py`** – A single file that implements:

  1. **CollaborativeKernelHub** (TCP broker on port 6000 + Flask dashboard on 5001)
  2. **NetworkedKernelTrainer** (each AI node)
  3. **CollaborativeTestHarness** (automated multi‐trainer scenarios)

* **Hub Responsibilities** (runs on port 6000):

  * Authenticate trainers (AI nodes) via a simple `<ai_id, token>` handshake.
  * Accept “HELLO” messages (specialization + capabilities), then broadcast a `TRAINING_SYNC`.
  * Persist shared patterns and components into SQLite (`hub_data.db`).
  * Relay feature requests, collaboration requests, and performance reports between trainers.
  * Host a minimal Flask dashboard (port 5001) to display real‐time stats.

* **Trainer Responsibilities**:

  * Connect to the hub, authenticate, and send a “HELLO.”
  * Simulate local training → broadcast dummy patterns and components.
  * Periodically request new collaborations and accept or reject based on a simple reputation system and specialization matching.
  * Send completed collaborative components back to partners.

* **Harness Responsibilities**:

  * Automatically launch one hub instance and multiple trainer instances in‐process.
  * Simulate a predefined scenario (e.g. “basic,” “multi\_spec,” “stress\_test,” “sequential”).
  * Poll the SQLite database every 30 seconds for updated statistics.
  * Print a JSON‐formatted summary when the scenario ends.

---

## 3. Launching the Hub + Dashboard

1. **Activate Your Virtual Environment** (if you created one earlier)

   ```bash
   cd /path/to/ColorOS
   source venv/bin/activate
   ```

2. **Run the Hub**

   ```bash
   python3 kernal.py hub
   ```

   * **Hub (TCP)** listens on port 6000.
   * **Dashboard (Flask)** runs on port 5001.

3. **Open the Dashboard in Your Browser**
   Visit:

   ```
   http://localhost:5001/
   ```

   You’ll see four live‐updating metrics:

   * **Connected AIs**
   * **Shared Patterns**
   * **Shared Components**
   * **Active Collaborations**

4. **Leave the Hub Terminal Open**
   You’ll observe incoming log lines each time a trainer connects, shares a pattern, or finishes a collaboration.

---

## 4. Running a Trainer (AI Node)

Each trainer needs three arguments:

```
python3 kernal.py trainer <AI_ID> <SPECIALIZATION> <TOKEN>
```

* **`<AI_ID>`** – A unique identifier for this AI (e.g. `AI_Node_1`).
* **`<SPECIALIZATION>`** – One of the built‐in roles:

  * `window_manager`
  * `file_system`
  * `process_manager`
  * `security`
  * `networking`
  * `graphics`
  * `audio`
  * `performance`
  * (Or any new key you add to `_determine_capabilities()` if you extend it.)
* **`<TOKEN>`** – Must match the token in the hub’s `auth_tokens` dictionary. By default, the file ships with:

  ```
  "AI_Node_1": "secret1"
  "AI_Node_2": "secret2"
  "AI_Node_3": "secret3"
  "AI_WindowManager": "secret1"
  … (and so on for predefined IDs)
  ```

### Example: Start Two Trainers

* In a new terminal (venv activated):

  ```bash
  cd /path/to/ColorOS
  source venv/bin/activate
  python3 kernal.py trainer AI_Node_1 networking secret1
  ```
* In another terminal:

  ```bash
  cd /path/to/ColorOS
  source venv/bin/activate
  python3 kernal.py trainer AI_Node_2 graphics secret2
  ```

**What You’ll Observe**:

* **Hub logs** show

  ```
  [+] AI Trainer authenticated: AI_Node_1 @ ('127.0.0.1', 52716)
  AI AI_Node_1 joined: specialization=networking, capabilities=[…]
  Pattern AI_Node_1_pattern_… shared by AI_Node_1
  OS component AI_Node_1_fs_module_… shared by AI_Node_1
  [then similar lines for AI_Node_2]
  Active collaborations increment as requests go out
  ```
* **Trainer logs** show

  ```
  [AI_Node_1] Connected to hub at 127.0.0.1:6000
  [AI_Node_1] Shared pattern AI_Node_1_pattern_…
  [AI_Node_1] Shared component AI_Node_1_fs_module_…
  [AI_Node_1] Broadcasted COLLABORATION_REQUEST
  ```
* **Dashboard** updates its counters in near‐real time.

---

## 5. Running an Automated Scenario

If you want to avoid manually launching multiple terminals, use the built‐in harness:

```bash
python3 kernal.py harness <scenario>
```

Available scenarios:

1. **`basic`**

   * **2 AIs** (networking + graphics)
   * Duration: 3 minutes
   * After 3 minutes, the harness prints a JSON summary and all processes exit.

2. **`multi_spec`**

   * **4 AIs** (networking, graphics, security, window\_manager)
   * Duration: 5 minutes

3. **`stress_test`**

   * **6 AIs** (overlapping specializations)
   * Duration: 8 minutes

4. **`sequential`**

   * **5 AIs** joining sequentially (30 s delays)
   * Duration: 10 minutes

### Example: Run the “basic” Scenario

```bash
source venv/bin/activate
python3 kernal.py harness basic
```

* The harness will:

  1. Spin up the hub + dashboard threads.
  2. Launch `AI_Node_1` immediately, then `AI_Node_2` after a 2 s delay.
  3. Let them run for 3 minutes, polling the hub’s SQLite counts every 30 s.
  4. After 3 minutes, both trainers disconnect, the harness collects final stats from SQLite, and prints a JSON object to stdout.

**Sample JSON Output**:

```json
{
  "scenario": "Basic Collaboration",
  "start_time": 1623312345.123,
  "trainers": {
    "AI_Node_1": {
      "results": {
        "session_id": "collab_training_1623312345",
        "duration_minutes": 3,
        "collaborations": [ … ],      // list of completed collab results
        "shared_patterns": 3,
        "shared_components": 2,
        "success": true
      },
      "success": true
    },
    "AI_Node_2": {
      "results": {
        "session_id": "collab_training_1623312347",
        "duration_minutes": 3,
        "collaborations": [ … ],
        "shared_patterns": 3,
        "shared_components": 2,
        "success": true
      },
      "success": true
    }
  },
  "hub_stats": {
    "t_30s": { "shared_patterns": 6, "shared_components": 4, "connected_ais": 2 },
    "t_60s": { "shared_patterns": 12, "shared_components": 6, "connected_ais": 2 },
    …
  },
  "final_stats": { "shared_patterns": 12, "shared_components": 8, "connected_ais": 2 },
  "end_time": 1623314123.456,
  "duration": 180.333
}
```

---

## 6. Inspecting SQLite Data

By default, the hub uses `hub_data.db` in the same directory. You can inspect it at any time:

```bash
sqlite3 hub_data.db
sqlite> .tables
shared_patterns  shared_components
sqlite> SELECT * FROM shared_patterns LIMIT 5;
sqlite> SELECT * FROM shared_components LIMIT 5;
sqlite> .quit
```

* **`shared_patterns` table** columns:

  * `pattern_id` (TEXT PRIMARY KEY)
  * `pattern_data` (JSON TEXT)
  * `contributor` (TEXT)
  * `timestamp` (TEXT ISO‐format)
  * `usage_count` (INTEGER)

* **`shared_components` table** columns:

  * `component_id` (TEXT PRIMARY KEY)
  * `component_data` (JSON TEXT)
  * `contributor` (TEXT)
  * `timestamp` (TEXT ISO‐format)
  * `quality_score` (REAL)
  * `usage_count` (INTEGER)

---

## 7. Customizing & Extending

1. **Add a New API Token**

   * Edit `kernal.py`’s `self.auth_tokens = { … }` dictionary near the top of `CollaborativeKernelHub.__init__`.
   * Add:

     ```python
     "AI_Custom": "mySecretX"
     ```
   * Restart the hub, then launch:

     ```bash
     python3 kernal.py trainer AI_Custom data_science mySecretX
     ```

2. **Define a New Specialization**

   * Locate `_determine_capabilities()` in `NetworkedKernelTrainer`.
   * Add a new key/value pair under `specs`, for example:

     ```python
     specs = {
         "window_manager": […],
         "file_system": […],
         "data_science": ["data_ingestion", "model_training", "analysis"]
     }
     ```
   * Restart the hub, then launch:

     ```bash
     python3 kernal.py trainer AI_DataScientist data_science secretX
     ```

3. **Adjust Reputation & Trust Threshold**

   * By default, each trainer’s constructor sets `trust_threshold=0.5` and `reputation_alpha=0.3`.
   * To make collaborators more or less discerning, pass custom values when launching manually (edit the trainer block at the bottom of `kernal.py`):

     ```bash
     python3 kernal.py trainer AI_Node_1 networking secret1  # uses default 0.5 threshold
     ```

     If you want to hardcode a lower threshold, open the `trainer` section in `kernal.py` and modify:

     ```python
     trainer = NetworkedKernelTrainer(
         ai_id="AI_Node_1",
         specialization="networking",
         hub_host="127.0.0.1",
         hub_port=6000,
         auth_token="secret1",
         trust_threshold=0.3,
         reputation_alpha=0.5
     )
     ```

4. **Plug in Your Own Training Code**

   * Replace the stubbed `IntegratedKernelTrainer` class with your real kernel‐training logic (e.g., deep‐learning model calls, data pipelines).
   * Ensure `run_comprehensive_demo()` returns a dictionary with at least a few numeric fields (e.g. `"patterns_learned"`, `"components_built"`, `"performance_gain"`).
   * The rest of the logic (pattern/component broadcasting, collaboration) will remain the same.

5. **Enhance the Flask Dashboard**

   * The current dashboard polls `/status` every 3 seconds and displays four numbers.
   * To show more detailed charts, edit the HTML template in the `@app.route("/")` method:

     ```python
     @flask_app.route("/")
     def dashboard():
         html = """
         <!DOCTYPE html><html><body>
           <h2>Collaborative Kernel Hub Dashboard</h2>
           <!-- Add Chart.js or D3 here -->
         </body></html>
         """
         return render_template_string(html)
     ```
   * Insert `<canvas>` elements, include Chart.js via CDN, and write JavaScript to fetch `/status` and redraw charts.

---

## 8. Troubleshooting & Tips

* **“Authentication failed”**

  * Double‐check that your `<AI_ID>` and `<TOKEN>` match the pair in `auth_tokens` exactly (case‐sensitive).
  * Restart the hub after editing `auth_tokens`.

* **Trainer never shows in Dashboard**

  * Ensure the trainer’s terminal logs “Connected to hub” (no exceptions).
  * Confirm the hub is still running on port 6000 and has not crashed.

* **SQLite errors**

  * If you see errors like `database is locked`, reduce simultaneous writes or switch to a more robust DB (PostgreSQL).
  * You can stop all processes, delete `hub_data.db`, and restart to get a fresh database.

* **Stuck Collaborations**

  * If two trainers request a collaboration but neither accepts, adjust `trust_threshold` or reduce the required complementarity in `_complements_specialization()`.
  * You can also temporarily set `trust_threshold=0.0` to force acceptance of all requests.

* **Slow Dashboard Refresh**

  * The default polling interval is 3 seconds; change the `setInterval(fetchStatus, 3000)` line to a shorter interval (e.g. 1000 ms) if you want faster updates.

---

## 9. Summary of Common Commands

```bash
# 1. Activate venv
cd /path/to/ColorOS
source venv/bin/activate

# 2. Install Flask + Matplotlib
pip install flask matplotlib

# 3. Start the hub + dashboard
python3 kernal.py hub

# 4. Start one or more trainers (each in its own terminal)
python3 kernal.py trainer AI_Node_1 networking secret1
python3 kernal.py trainer AI_Node_2 graphics   secret2

# 5. (Optional) Run an automated scenario:
python3 kernal.py harness basic
python3 kernal.py harness multi_spec
python3 kernal.py harness stress_test
python3 kernal.py harness sequential

# 6. Inspect the SQLite database at any time
sqlite3 hub_data.db
  sqlite> SELECT COUNT(*) FROM shared_patterns;
  sqlite> SELECT COUNT(*) FROM shared_components;
  sqlite> .quit
```

---

### You’re All Set!

With these instructions, new users can **quickly**:

1. Install dependencies
2. Launch the hub + dashboard
3. Connect multiple trainers
4. Observe real‐time statistics
5. Run automated test scenarios
6. Extend or customize specializations, reputation rules, and the dashboard

Feel free to copy these steps into your project’s `README.md` or internal documentation. Happy collaborating!
