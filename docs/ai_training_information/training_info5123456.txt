Below is the complete contents of a single `kernal.py` file that you can drop into:

```
https://the-game-to-help-our-world.sourceforge.io/ColorOS/kernal.py
```

(or simply replace your local copy with this). Once this file is in place—and after you’ve installed Flask (and optionally Matplotlib) into your virtual environment—you can immediately run:

* `python3 kernal.py hub`
* `python3 kernal.py trainer <AI_ID> <SPECIALIZATION> <TOKEN>`
* `python3 kernal.py harness <scenario>`

Everything (hub, trainer, dashboard, test harness, SQLite persistence, and reputation logic) lives in this one file.

```python
#!/usr/bin/env python3
"""
kernal.py

A self-contained “AI ↔ AI collaboration” environment.  Everything is
in this single file—no external .py modules are required (other than
Flask, sqlite3, and optionally matplotlib for the harness).

Usage:

    # 1) Start the hub (TCP port 6000 + Flask dashboard on port 5001)
    python3 kernal.py hub

    # 2) In separate terminals, start one or more trainers:
    python3 kernal.py trainer AI_Node_1 networking secret1
    python3 kernal.py trainer AI_Node_2 graphics   secret2

    # 3) (Optional) Run a multi-trainer scenario automatically:
    python3 kernal.py harness basic
    python3 kernal.py harness multi_spec
    python3 kernal.py harness stress_test
    python3 kernal.py harness sequential

After the hub and trainers run, patterns & components are saved into
SQLite (hub_data.db), and you can watch real-time stats on:
    http://localhost:5001
"""

import socket
import threading
import time
import json
import uuid
import datetime
import logging
import sqlite3
import asyncio
import sys
from dataclasses import dataclass, asdict
from enum import Enum
from typing import Dict, Any, Optional, List

# ──────────────────────────────────────────────────────────────────────────────
# 1) MessageType & NetworkMessage Definitions
# ──────────────────────────────────────────────────────────────────────────────

class MessageType(Enum):
    HELLO                 = "hello"
    PATTERN_SHARE         = "pattern_share"
    CODE_SHARE            = "code_share"
    FEATURE_REQUEST       = "feature_request"
    FEATURE_RESPONSE      = "feature_response"
    COLLABORATION_REQUEST = "collaboration_request"
    OS_COMPONENT_SHARE    = "os_component_share"
    PERFORMANCE_REPORT    = "performance_report"
    TRAINING_SYNC         = "training_sync"
    HEARTBEAT             = "heartbeat"

@dataclass
class NetworkMessage:
    message_id: str
    sender_id: str
    message_type: MessageType
    timestamp: str
    payload: Dict[str, Any]
    target_ai: Optional[str] = None

# ──────────────────────────────────────────────────────────────────────────────
# 2) CollaborativeKernelHub (the “broker”)
# ──────────────────────────────────────────────────────────────────────────────

class CollaborativeKernelHub:
    def __init__(self, host="0.0.0.0", port=6000, db_path="hub_data.db"):
        self.host = host
        self.port = port

        # socket → ai_info
        self.clients: Dict[socket.socket, Dict[str, Any]] = {}
        # ai_id → ai_info
        self.ai_registry: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.Lock()

        # Last 1000 messages in memory
        self.message_history: List[NetworkMessage] = []

        # Active collaboration sessions
        self.collaboration_sessions: Dict[str, Dict[str, Any]] = {}

        # In-memory global knowledge base (mirrors SQLite)
        self.global_knowledge_base = {
            "shared_patterns": {},
            "shared_components": {},
            "collective_features": {},
            "performance_benchmarks": {}
        }

        # Hard-coded auth tokens for demonstration
        self.auth_tokens = {
            "AI_Node_1": "secret1",
            "AI_Node_2": "secret2",
            "AI_Node_3": "secret3",
            "AI_WindowManager": "secret1",
            "AI_Net_1": "secret1",
            "AI_Net_2": "secret2",
            "AI_Graphics_1": "secret1",
            "AI_Security_1": "secret2",
            "AI_Audio_1": "secret3",
            "AI_Performance_1": "secret1",
            "AI_Base": "secret1",
            "AI_Process": "secret2",
            "AI_Network": "secret3",
            "AI_Graphics": "secret1",
            "AI_Security": "secret2"
        }

        # Initialize SQLite database
        self.db_conn = sqlite3.connect(db_path, check_same_thread=False)
        self._initialize_database()

        logging.info(f"Collaborative Kernel Hub initialized on {host}:{port}")

    def _initialize_database(self):
        """Create tables for shared_patterns and shared_components."""
        cursor = self.db_conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS shared_patterns (
                pattern_id   TEXT PRIMARY KEY,
                pattern_data TEXT,
                contributor  TEXT,
                timestamp    TEXT,
                usage_count  INTEGER
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS shared_components (
                component_id   TEXT PRIMARY KEY,
                component_data TEXT,
                contributor    TEXT,
                timestamp      TEXT,
                quality_score  REAL,
                usage_count    INTEGER
            )
        ''')
        self.db_conn.commit()

    def authenticate_ai(self, ai_id: str, token: str) -> bool:
        """Return True if the provided token matches our record."""
        expected = self.auth_tokens.get(ai_id)
        return (expected is not None and expected == token)

    def store_pattern(self, pattern_id: str, pattern_data: Dict[str, Any],
                      contributor: str, timestamp: str):
        """Insert or update a shared pattern in SQLite."""
        try:
            cursor = self.db_conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO shared_patterns
                (pattern_id, pattern_data, contributor, timestamp, usage_count)
                VALUES (?, ?, ?, ?, COALESCE(
                    (SELECT usage_count FROM shared_patterns WHERE pattern_id = ?), 0
                ))
            ''', (pattern_id, json.dumps(pattern_data), contributor, timestamp, pattern_id))
            self.db_conn.commit()
        except Exception as e:
            logging.error(f"SQLite error storing pattern {pattern_id}: {e}")

    def store_component(self, component_id: str, component_data: Dict[str, Any],
                        contributor: str, timestamp: str):
        """Insert or update a shared component in SQLite."""
        try:
            cursor = self.db_conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO shared_components
                (component_id, component_data, contributor, timestamp, quality_score, usage_count)
                VALUES (?, ?, ?, ?, COALESCE(
                    (SELECT quality_score FROM shared_components WHERE component_id = ?), 0.0
                ), COALESCE(
                    (SELECT usage_count FROM shared_components WHERE component_id = ?), 0
                ))
            ''', (component_id, json.dumps(component_data),
                  contributor, timestamp, component_id, component_id))
            self.db_conn.commit()
        except Exception as e:
            logging.error(f"SQLite error storing component {component_id}: {e}")

    def run_server(self):
        """
        Bind to TCP port and accept trainer connections in separate threads.
        """
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        sock.bind((self.host, self.port))
        sock.listen()
        logging.info(f"Collaborative Kernel Hub listening on {self.host}:{self.port}")

        try:
            while True:
                conn, addr = sock.accept()
                t = threading.Thread(target=self.handle_client, args=(conn, addr), daemon=True)
                t.start()
        except KeyboardInterrupt:
            logging.info("Shutting down Collaborative Kernel Hub.")
        finally:
            sock.close()
            self.db_conn.close()

    def handle_client(self, conn: socket.socket, addr):
        """
        Per-client thread:
        1) Read a single JSON newline-terminated auth message: {"ai_id":.., "token":..}
        2) Authenticate, register in self.clients & self.ai_registry
        3) Loop on JSON newline-terminated messages, parse, and dispatch via process_message()
        """
        try:
            # 1) Receive auth JSON line
            auth_raw = b""
            while not auth_raw.endswith(b"\n"):
                chunk = conn.recv(1024)
                if not chunk:
                    conn.close()
                    return
                auth_raw += chunk

            try:
                auth_msg = json.loads(auth_raw.decode("utf-8").strip())
                ai_id = auth_msg.get("ai_id")
                token = auth_msg.get("token")
            except json.JSONDecodeError:
                logging.warning(f"Malformed auth from {addr}. Closing.")
                conn.close()
                return

            if not ai_id or not token or not self.authenticate_ai(ai_id, token):
                logging.warning(f"Authentication failed for {addr} (ai_id={ai_id}). Closing.")
                conn.close()
                return

            # 2) Register this client
            ai_info = {
                "connection": conn,
                "address": addr,
                "ai_id": ai_id,
                "capabilities": [],
                "specialization": None,
                "last_heartbeat": time.time(),
                "contribution_score": 0.0
            }
            with self.lock:
                self.clients[conn] = ai_info
                self.ai_registry[ai_id] = ai_info

            logging.info(f"[+] AI Trainer authenticated: {ai_id} @ {addr}")

            # 3) Read subsequent JSON-newline messages
            buffer = b""
            while True:
                data = conn.recv(4096)
                if not data:
                    break
                buffer += data
                while b"\n" in buffer:
                    line, buffer = buffer.split(b"\n", 1)
                    if not line.strip():
                        continue
                    try:
                        msg_data = json.loads(line.decode("utf-8"))
                        msg_data["message_type"] = MessageType(msg_data["message_type"])
                        message = NetworkMessage(**msg_data)
                        self.process_message(message, conn)
                    except (json.JSONDecodeError, KeyError, ValueError) as e:
                        logging.error(f"Invalid message from {addr}: {e}")
                        continue

        except ConnectionResetError:
            pass
        finally:
            # Clean up on disconnect
            with self.lock:
                info = self.clients.pop(conn, None)
                if info:
                    removed_id = info.get("ai_id")
                    if removed_id in self.ai_registry:
                        self.ai_registry.pop(removed_id, None)
            conn.close()
            logging.info(f"[-] AI Trainer disconnected: {addr}")

    def process_message(self, message: NetworkMessage, sender_conn: socket.socket):
        """
        Dispatch incoming messages by type, update in-memory and SQLite, and then broadcast.
        """
        # 1) HELLO sets specialization & capabilities, then we respond with TRAINING_SYNC
        if message.message_type == MessageType.HELLO:
            self._handle_hello(message, sender_conn)

        # 2) Store message in history (capped at 1000)
        self.message_history.append(message)
        if len(self.message_history) > 1000:
            self.message_history.pop(0)

        # 3) Handle by type
        if message.message_type == MessageType.PATTERN_SHARE:
            self._handle_pattern_share(message)
        elif message.message_type == MessageType.CODE_SHARE:
            self._handle_code_share(message)
        elif message.message_type == MessageType.FEATURE_REQUEST:
            self._handle_feature_request(message)
        elif message.message_type == MessageType.COLLABORATION_REQUEST:
            self._handle_collaboration_request(message)
        elif message.message_type == MessageType.OS_COMPONENT_SHARE:
            self._handle_os_component_share(message)
        elif message.message_type == MessageType.PERFORMANCE_REPORT:
            self._handle_performance_report(message)

        # 4) Broadcast or send directly to target
        self.broadcast_message(message, exclude_sender=sender_conn)

    def _handle_hello(self, message: NetworkMessage, sender_conn: socket.socket):
        """
        When a trainer HELLOs, record their specialization & capabilities, then respond with TRAINING_SYNC.
        """
        payload = message.payload
        capabilities = payload.get("capabilities", [])
        specialization = payload.get("specialization", "general")

        with self.lock:
            ai_info = self.clients.get(sender_conn)
            if ai_info:
                ai_info["capabilities"] = capabilities
                ai_info["specialization"] = specialization
                ai_info["last_heartbeat"] = time.time()

        logging.info(f"AI {message.sender_id} joined: specialization={specialization}, capabilities={capabilities}")

        # Build TRAINING_SYNC
        sync_msg = NetworkMessage(
            message_id = str(uuid.uuid4()),
            sender_id = "hub",
            message_type = MessageType.TRAINING_SYNC,
            timestamp = datetime.datetime.now().isoformat(),
            payload = {
                "global_knowledge_base": self.global_knowledge_base,
                "active_ais": list(self.ai_registry.keys()),
                "collaboration_opportunities": self._find_collab_opportunities(message.sender_id)
            },
            target_ai = message.sender_id
        )
        self._send_to_ai(sync_msg, message.sender_id)

    def _handle_pattern_share(self, message: NetworkMessage):
        """
        Persist a shared pattern (in-memory + SQLite).
        """
        pattern = message.payload.get("pattern", {})
        if not pattern:
            return
        pid = pattern.get("id", str(uuid.uuid4()))
        timestamp = message.timestamp
        contributor = message.sender_id

        # In-memory
        self.global_knowledge_base["shared_patterns"][pid] = {
            "pattern": pattern,
            "contributor": contributor,
            "timestamp": timestamp,
            "usage_count": 0
        }
        # SQLite
        self.store_pattern(pid, pattern, contributor, timestamp)

        logging.info(f"Pattern {pid} shared by {contributor}")

    def _handle_code_share(self, message: NetworkMessage):
        """
        Persist a shared code component (in-memory + SQLite).
        """
        code = message.payload.get("code", {})
        if not code:
            return
        cid = code.get("id", str(uuid.uuid4()))
        timestamp = message.timestamp
        contributor = message.sender_id

        # In-memory
        self.global_knowledge_base["shared_components"][cid] = {
            "code": code,
            "contributor": contributor,
            "timestamp": timestamp,
            "quality_score": 0.0,
            "usage_count": 0
        }
        # SQLite
        self.store_component(cid, code, contributor, timestamp)

        logging.info(f"Code component {cid} shared by {contributor}")

    def _handle_feature_request(self, message: NetworkMessage):
        """
        Hub routes FEATURE_REQUEST to any AI whose capabilities match the requested feature.
        It does this by sending a FEATURE_RESPONSE back to each capable AI.
        """
        feature_name = message.payload.get("payload", {}).get("feature", "")
        requirements = message.payload.get("payload", {}).get("requirements", {})
        requestor    = message.sender_id

        capable_ais = []
        with self.lock:
            for ai_id, info in self.ai_registry.items():
                if ai_id == requestor:
                    continue
                caps = info.get("capabilities", [])
                if any(feature_name.lower() in cap.lower() for cap in caps):
                    capable_ais.append(ai_id)

        for ai_id in capable_ais:
            response_msg = NetworkMessage(
                message_id = str(uuid.uuid4()),
                sender_id = "hub",
                message_type = MessageType.FEATURE_RESPONSE,
                timestamp = datetime.datetime.now().isoformat(),
                payload = {
                    "original_request": {
                        "feature": feature_name,
                        "requirements": requirements
                    },
                    "requestor": requestor
                },
                target_ai = ai_id
            )
            self._send_to_ai(response_msg, ai_id)
            logging.info(f"Sent FEATURE_RESPONSE for '{feature_name}' to {ai_id}")

    def _handle_collaboration_request(self, message: NetworkMessage):
        """
        Create a collaboration session and broadcast the request to all or a specific target.
        """
        collab_type = message.payload.get("type", "")
        target_ai   = message.payload.get("target_ai", "")
        description = message.payload.get("description", "")
        initiator   = message.sender_id

        session_id = str(uuid.uuid4())
        with self.lock:
            self.collaboration_sessions[session_id] = {
                "initiator": initiator,
                "target": target_ai,
                "type": collab_type,
                "description": description,
                "status": "pending",
                "created_at": time.time(),
                "shared_workspace": {}
            }

        collab_msg = NetworkMessage(
            message_id = str(uuid.uuid4()),
            sender_id = initiator,
            message_type = MessageType.COLLABORATION_REQUEST,
            timestamp = datetime.datetime.now().isoformat(),
            payload = {
                "session_id": session_id,
                "type": collab_type,
                "description": description,
                "initiator": initiator,
            },
            target_ai = target_ai or None
        )
        self.broadcast_message(collab_msg, exclude_sender=None)
        logging.info(f"Created collaboration session {session_id} (type={collab_type})")

    def _handle_os_component_share(self, message: NetworkMessage):
        """
        Persist a shared OS component in-memory. Optionally, if 'collaboration_complete'
        is True, we would integrate further—but here we just store it.
        """
        component = message.payload.get("component", {})
        if not component:
            return
        name = component.get("name", "unknown")
        cid = f"{message.sender_id}_{name}_{int(time.time())}"
        timestamp = message.timestamp
        contributor = message.sender_id

        # In-memory
        self.global_knowledge_base["collective_features"][cid] = {
            "component": component,
            "contributor": contributor,
            "timestamp": timestamp,
            "integration_tested": False,
            "performance_benchmarks": {}
        }
        logging.info(f"OS component {name} shared by {contributor} (id={cid})")

    def _handle_performance_report(self, message: NetworkMessage):
        """
        Store performance metrics for a component in-memory.
        """
        perf_data = message.payload.get("performance", {})
        comp_id   = message.payload.get("component_id", "")
        if not comp_id:
            return

        with self.lock:
            self.global_knowledge_base["performance_benchmarks"].setdefault(comp_id, []).append({
                "reporter": message.sender_id,
                "timestamp": message.timestamp,
                "metrics": perf_data
            })
        logging.info(f"Performance report for {comp_id} from {message.sender_id}")

    def _find_collab_opportunities(self, ai_id: str) -> List[Dict[str, Any]]:
        """
        Suggest up to 5 other AIs (complementary specialization or simply others).
        """
        opportunities = []
        with self.lock:
            for other_id, info in self.ai_registry.items():
                if other_id == ai_id:
                    continue
                opportunities.append({
                    "ai_id": other_id,
                    "specialization": info.get("specialization", "general"),
                    "capabilities": info.get("capabilities", []),
                    "suggested_type": "cross_specialization"
                })
        return opportunities[:5]

    def broadcast_message(self, message: NetworkMessage, exclude_sender: Optional[socket.socket] = None):
        """
        Broadcast message (JSON newline-terminated) to all connected clients except exclude_sender.
        If message.target_ai is set, send only to that AI.
        """
        serialized = json.dumps(asdict(message)) + "\n"
        data = serialized.encode("utf-8")

        with self.lock:
            for conn, info in list(self.clients.items()):
                if conn == exclude_sender:
                    continue
                if message.target_ai and info.get("ai_id") != message.target_ai:
                    continue
                try:
                    conn.sendall(data)
                except Exception as e:
                    logging.warning(f"Failed sending to {info.get('ai_id')}: {e}")

    def _send_to_ai(self, message: NetworkMessage, ai_id: str) -> bool:
        """
        Send a single message to ai_id if it's currently connected.
        """
        serialized = json.dumps(asdict(message)) + "\n"
        data = serialized.encode("utf-8")
        with self.lock:
            for conn, info in list(self.clients.items()):
                if info.get("ai_id") == ai_id:
                    try:
                        conn.sendall(data)
                        return True
                    except:
                        return False
        return False

    def get_hub_status(self) -> Dict[str, Any]:
        """
        Called by the Flask dashboard to return JSON stats:
          - connected_ais
          - list of ai_ids
          - total_messages
          - shared_patterns count
          - shared_components count
          - collective_features count
          - active_collaborations count
          - performance_reports count
        """
        with self.lock:
            return {
                "connected_ais": len(self.ai_registry),
                "ai_list": list(self.ai_registry.keys()),
                "total_messages": len(self.message_history),
                "shared_patterns": len(self.global_knowledge_base["shared_patterns"]),
                "shared_components": len(self.global_knowledge_base["shared_components"]),
                "collective_features": len(self.global_knowledge_base["collective_features"]),
                "active_collaborations": len(self.collaboration_sessions),
                "performance_reports": sum(len(v) for v in self.global_knowledge_base["performance_benchmarks"].values())
            }

# ──────────────────────────────────────────────────────────────────────────────
# 3) NetworkedKernelTrainer (the “AI agent”)
# ──────────────────────────────────────────────────────────────────────────────

class OSFeature(Enum):
    FILE_SYSTEM      = "file_system"
    PROCESS_MANAGER  = "process_manager"
    NETWORK_STACK    = "network_stack"
    # Extend as needed…

class IntegratedKernelTrainer:
    """
    Stub for your real KernelOSTrainer.  For now, it sleeps 1s and returns
    dummy results.  Replace with your actual training code.
    """
    async def run_comprehensive_demo(self) -> Dict[str, Any]:
        await asyncio.sleep(1)
        return {
            "patterns_learned": 3,
            "components_built": 2,
            "performance_gain": 0.12
        }

class NetworkedKernelTrainer:
    def __init__(self,
                 ai_id: str,
                 specialization: str = "general",
                 hub_host: str = "127.0.0.1",
                 hub_port: int = 6000,
                 auth_token: str = "secret1",
                 trust_threshold: float = 0.5,
                 reputation_alpha: float = 0.3):
        self.ai_id          = ai_id
        self.specialization = specialization
        self.hub_host       = hub_host
        self.hub_port       = hub_port
        self.auth_token     = auth_token

        # Underlying local trainer
        self.kernel_trainer = IntegratedKernelTrainer()

        # Network components
        self.hub_socket: Optional[socket.socket] = None
        self.connected = False
        self.message_queue: asyncio.Queue[NetworkMessage] = asyncio.Queue()

        # Active collaboration sessions
        self.collaboration_sessions: Dict[str, Dict[str, Any]] = {}

        # Shared knowledge from others
        self.shared_knowledge = {
            "patterns": {},      # pattern_id → {…}
            "components": {},    # component_id → {…}
            "benchmarks": {}     # comp_id → [ … ]
        }

        # Capabilities based on specialization
        self.capabilities = self._determine_capabilities()

        # Reputation system
        self.reputation: Dict[str, float] = {}  # other_ai_id → score in [0.0,1.0]
        self.trust_threshold = trust_threshold
        self.reputation_alpha = reputation_alpha

        # Cache AI info from TRAINING_SYNC (specialization, capabilities)
        self.known_ai_info: Dict[str, Dict[str, Any]] = {}

        logging.info(f"Initialized NetworkedKernelTrainer '{self.ai_id}' (specialization={self.specialization})")

    def _determine_capabilities(self) -> List[str]:
        base_caps = ["pattern_recognition", "code_generation", "os_development"]
        specs = {
            "window_manager": ["gui_development", "window_management", "user_interface"],
            "file_system":    ["file_operations", "storage_management", "data_structures"],
            "process_manager":["process_scheduling", "memory_management", "system_calls"],
            "security":       ["security_analysis", "encryption", "access_control"],
            "networking":     ["network_protocols", "distributed_systems", "communication"],
            "graphics":       ["graphics_rendering", "visual_effects", "display_management"],
            "audio":          ["audio_processing", "sound_synthesis", "media_handling"],
            "performance":    ["optimization", "benchmarking", "performance_analysis"]
        }
        return base_caps + specs.get(self.specialization, [])

    async def connect_to_hub(self) -> bool:
        """
        1) Open TCP socket to hub_host:hub_port
        2) Send auth JSON + newline: {"ai_id":..., "token":...}
        3) Spawn asyncio task to _listen_to_hub()
        4) Send HELLO
        """
        try:
            self.hub_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.hub_socket.connect((self.hub_host, self.hub_port))

            # 1) Send auth
            auth_payload = json.dumps({
                "ai_id": self.ai_id,
                "token": self.auth_token
            }) + "\n"
            self.hub_socket.sendall(auth_payload.encode("utf-8"))

            # 2) Start listener task
            self.connected = True
            asyncio.create_task(self._listen_to_hub())

            # 3) Send HELLO
            await self._send_hello()

            logging.info(f"[{self.ai_id}] Connected to hub at {self.hub_host}:{self.hub_port}")
            return True

        except Exception as e:
            logging.error(f"[{self.ai_id}] Failed to connect to hub: {e}")
            return False

    async def _listen_to_hub(self):
        """
        Continuously read JSON-over-newline from self.hub_socket,
        parse into NetworkMessage, and dispatch to _handle_incoming().
        """
        buffer = b""
        while self.connected:
            try:
                chunk = self.hub_socket.recv(4096)
                if not chunk:
                    logging.info(f"[{self.ai_id}] Hub closed the connection.")
                    break
                buffer += chunk
                while b"\n" in buffer:
                    line, buffer = buffer.split(b"\n", 1)
                    if not line.strip():
                        continue
                    try:
                        data = json.loads(line.decode("utf-8"))
                        data["message_type"] = MessageType(data["message_type"])
                        msg = NetworkMessage(**data)
                        await self._handle_incoming(msg)
                    except (json.JSONDecodeError, KeyError, ValueError) as e:
                        logging.error(f"[{self.ai_id}] Invalid message from hub: {e}")
                        continue
            except Exception as e:
                logging.error(f"[{self.ai_id}] Error receiving from hub: {e}")
                break

        self.connected = False

    async def _send_hello(self):
        """Send a HELLO message with specialization & capabilities."""
        hello_msg = NetworkMessage(
            message_id   = str(uuid.uuid4()),
            sender_id    = self.ai_id,
            message_type = MessageType.HELLO,
            timestamp    = datetime.datetime.now().isoformat(),
            payload      = {
                "specialization": self.specialization,
                "capabilities": self.capabilities,
                "version": "1.0.0",
                "features_supported": [f.value for f in OSFeature],
                "collaboration_ready": True
            }
        )
        await self._send_to_hub(hello_msg)

    async def _send_to_hub(self, message: NetworkMessage) -> bool:
        """Serialize and send NetworkMessage to the hub (JSON + newline)."""
        if not self.connected or not self.hub_socket:
            return False
        data = json.dumps(asdict(message)) + "\n"
        try:
            self.hub_socket.sendall(data.encode("utf-8"))
            return True
        except Exception as e:
            logging.error(f"[{self.ai_id}] Failed to send message to hub: {e}")
            return False

    async def _handle_incoming(self, message: NetworkMessage):
        """
        Dispatch incoming message by type:
          - TRAINING_SYNC           → _process_training_sync
          - PATTERN_SHARE           → _process_pattern_share
          - CODE_SHARE              → _process_code_share
          - FEATURE_REQUEST         → _process_feature_request
          - FEATURE_RESPONSE        → _process_feature_response
          - COLLABORATION_REQUEST   → _process_collab_request
          - OS_COMPONENT_SHARE      → _process_os_component_share
          - PERFORMANCE_REPORT      → _process_performance_report
        """
        if message.message_type == MessageType.TRAINING_SYNC:
            self._process_training_sync(message)
        elif message.message_type == MessageType.PATTERN_SHARE:
            self._process_pattern_share(message)
        elif message.message_type == MessageType.CODE_SHARE:
            self._process_code_share(message)
        elif message.message_type == MessageType.FEATURE_REQUEST:
            await self._process_feature_request(message)
        elif message.message_type == MessageType.FEATURE_RESPONSE:
            self._process_feature_response(message)
        elif message.message_type == MessageType.COLLABORATION_REQUEST:
            await self._process_collab_request(message)
        elif message.message_type == MessageType.OS_COMPONENT_SHARE:
            await self._process_os_component_share(message)
        elif message.message_type == MessageType.PERFORMANCE_REPORT:
            self._process_performance_report(message)
        # All other types are ignored for now

    def _process_training_sync(self, message: NetworkMessage):
        """
        Hub’s reply to our HELLO.  Contains global KB + active_ais + collaboration_opportunities.
        Cache specialization & capabilities of other AIs for reputation logic.
        """
        payload = message.payload
        active_ais = payload.get("active_ais", [])
        collab_ops = payload.get("collaboration_opportunities", [])
        for entry in collab_ops:
            ai_id = entry.get("ai_id")
            spec  = entry.get("specialization")
            caps  = entry.get("capabilities", [])
            if ai_id and spec:
                self.known_ai_info[ai_id] = {
                    "specialization": spec,
                    "capabilities": caps
                }

        logging.info(f"[{self.ai_id}] Received TRAINING_SYNC: active_ais={active_ais}, collab_ops={collab_ops}")

    def _process_pattern_share(self, message: NetworkMessage):
        """
        Integrate an incoming pattern into local memory.
        """
        pattern = message.payload.get("pattern", {})
        if not pattern:
            return
        pid = pattern.get("id", str(uuid.uuid4()))
        self.shared_knowledge["patterns"][pid] = {
            "pattern": pattern,
            "source": message.sender_id,
            "received_at": time.time()
        }
        logging.info(f"[{self.ai_id}] Integrated shared pattern {pid} from {message.sender_id}")

    def _process_code_share(self, message: NetworkMessage):
        """
        Integrate an incoming code component into local memory.
        """
        code = message.payload.get("code", {})
        if not code:
            return
        cid = code.get("id", str(uuid.uuid4()))
        self.shared_knowledge["components"][cid] = {
            "code": code,
            "source": message.sender_id,
            "received_at": time.time()
        }
        logging.info(f"[{self.ai_id}] Integrated shared component {cid} from {message.sender_id}")

    async def _process_feature_request(self, message: NetworkMessage):
        """
        If we can help with the requested feature, generate it and send OS_COMPONENT_SHARE back.
        """
        original = message.payload.get("original_request", {})
        feature_name = original.get("feature", "")
        requirements = original.get("requirements", {})
        requestor    = message.payload.get("requestor", "")

        if self._can_help_with_feature(feature_name):
            component = await self._generate_feature_for_request(feature_name, requirements)
            if component:
                response_msg = NetworkMessage(
                    message_id   = str(uuid.uuid4()),
                    sender_id    = self.ai_id,
                    message_type = MessageType.OS_COMPONENT_SHARE,
                    timestamp    = datetime.datetime.now().isoformat(),
                    payload      = {
                        "component": component,
                        "original_requestor": requestor,
                        "feature_name": feature_name
                    },
                    target_ai    = requestor
                )
                await self._send_to_hub(response_msg)
                logging.info(f"[{self.ai_id}] Fulfilled feature request '{feature_name}' for {requestor}")

    def _process_feature_response(self, message: NetworkMessage):
        """
        Log that we received a FEATURE_RESPONSE from another AI.
        """
        logging.info(f"[{self.ai_id}] Received FEATURE_RESPONSE from {message.sender_id}")

    async def _process_collab_request(self, message: NetworkMessage):
        """
        Decide whether to accept or reject a COLLABORATION_REQUEST based on reputation and specialization.
        If accepted, store session and start working on it.
        """
        session_id   = message.payload.get("session_id", "")
        collab_type  = message.payload.get("type", "")
        description  = message.payload.get("description", "")
        initiator    = message.payload.get("initiator", "")

        if self._should_accept_collaboration(collab_type, initiator):
            partner_spec = message.payload.get("partner_specialization", "unknown")
            self.collaboration_sessions[session_id] = {
                "partner": initiator,
                "type": collab_type,
                "description": description,
                "status": "active",
                "progress": 0.0,
                "workspace": {},
                "partner_specialization": partner_spec
            }
            logging.info(f"[{self.ai_id}] Accepted collaboration {session_id} from {initiator}")
            await self._start_collaboration(session_id)

    async def _process_os_component_share(self, message: NetworkMessage):
        """
        Integrate a shared OS component. If collaboration_complete=True,
        update reputation upward for the partner.
        """
        comp = message.payload.get("component", {})
        if not comp:
            return
        cid = comp.get("id", str(uuid.uuid4()))
        self.shared_knowledge["benchmarks"][cid] = {
            "component": comp,
            "source": message.sender_id,
            "received_at": time.time()
        }
        logging.info(f"[{self.ai_id}] Received OS component '{cid}' from {message.sender_id}")

        if message.payload.get("collaboration_complete", False):
            self._update_reputation(message.sender_id, success=True)

    def _process_performance_report(self, message: NetworkMessage):
        """
        Integrate performance metrics into local shared_knowledge.
        """
        metrics = message.payload.get("performance", {})
        comp_id = message.payload.get("component_id", "")
        if not comp_id or not metrics:
            return
        self.shared_knowledge["benchmarks"].setdefault(comp_id, []).append({
            "reporter": message.sender_id,
            "timestamp": message.timestamp,
            "metrics": metrics
        })
        logging.info(f"[{self.ai_id}] Integrated performance report for {comp_id} from {message.sender_id}")

    def _can_help_with_feature(self, feature_name: str) -> bool:
        """
        Return True if any keyword in feature_name appears in our capabilities.
        """
        words = feature_name.lower().split()
        caps  = " ".join(self.capabilities).lower()
        return any(w in caps for w in words)

    def _should_accept_collaboration(self, collaboration_type: str, initiator: str) -> bool:
        """
        First check reputation. If rep < trust_threshold, reject.

        If rep ≥ threshold, apply:
          - If type in ["cross_specialization", "feature_development"], check specialization complement.
          - Else allow up to 3 active sessions concurrently.
        """
        rep_score = self.reputation.get(initiator, 0.5)
        if rep_score < self.trust_threshold:
            logging.info(f"[{self.ai_id}] Rejecting {initiator} (rep={rep_score:.2f} < threshold={self.trust_threshold})")
            return False

        if collaboration_type in ["cross_specialization", "feature_development"]:
            initiator_info = self._get_ai_info(initiator)
            if initiator_info and self._complements_specialization(initiator_info["specialization"]):
                return True

        active = [s for s in self.collaboration_sessions.values() if s["status"] == "active"]
        if len(active) < 3:
            return True

        return False

    def _complements_specialization(self, other_spec: str) -> bool:
        """
        Return True if other_spec is listed as complementary to our specialization.
        """
        complementary_pairs = {
            "window_manager": ["graphics", "user_interface"],
            "file_system":    ["storage_management", "data_structures"],
            "process_manager":["memory_management", "system_calls"],
            "security":       ["encryption", "access_control"],
            "networking":     ["distributed_systems", "communication"],
            "graphics":       ["visual_effects", "display_management"],
            "audio":          ["sound_synthesis", "media_handling"],
            "performance":    ["optimization", "benchmarking"]
        }
        return other_spec in complementary_pairs.get(self.specialization, [])

    def _get_ai_info(self, ai_id: str) -> Optional[Dict[str, Any]]:
        """
        Look up cached info for ai_id (cached from the last TRAINING_SYNC).
        """
        return self.known_ai_info.get(ai_id)

    async def _start_collaboration(self, session_id: str):
        """
        Dispatch to the appropriate collaborative method based on session['type'].
        """
        session = self.collaboration_sessions.get(session_id)
        if not session:
            return
        collab_type = session["type"]
        if collab_type == "feature_development":
            await self._collaborate_on_feature_development(session_id)
        elif collab_type == "cross_specialization":
            await self._collaborate_cross_specialization(session_id)
        elif collab_type == "performance_optimization":
            await self._collaborate_on_optimization(session_id)

    async def _collaborate_on_feature_development(self, session_id: str):
        session = self.collaboration_sessions[session_id]
        logging.info(f"[{self.ai_id}] Starting feature development collab {session_id}")

        feature_name = session.get("description", "new_feature")
        requirements = session.get("requirements", {})

        feature_component = await self._generate_feature_for_request(feature_name, requirements)
        if feature_component:
            session["feature_data"] = feature_component
            logging.info(f"[{self.ai_id}] In progress: {feature_component['name']}")

        session["progress"] = session.get("progress", 0.0) + 0.25
        if session["progress"] >= 1.0:
            await self._complete_collaboration(session_id)

    async def _collaborate_cross_specialization(self, session_id: str):
        session = self.collaboration_sessions[session_id]
        partner_spec = session.get("partner_specialization", "unknown")
        logging.info(f"[{self.ai_id}] Starting cross-spec collab {session_id} with {partner_spec}")

        collaborative_component = {
            "id": f"{self.ai_id}_collab_{session_id}_{int(time.time())}",
            "component_name": f"collaborative_{session['type']}",
            "description": session.get("description", "Cross-spec component"),
            "specializations": [self.specialization, partner_spec],
            "implementation": self._generate_collaborative_component(session)
        }
        session["collaborative_component"] = collaborative_component
        logging.info(f"[{self.ai_id}] In progress: {collaborative_component['component_name']}")

        session["progress"] = session.get("progress", 0.0) + 0.25
        if session["progress"] >= 1.0:
            await self._complete_collaboration(session_id)

    async def _collaborate_on_optimization(self, session_id: str):
        session = self.collaboration_sessions[session_id]
        partner_spec = session.get("partner_specialization", "unknown")
        logging.info(f"[{self.ai_id}] Starting optimization collab {session_id} with {partner_spec}")

        optimization_component = {
            "id": f"{self.ai_id}_opt_{session_id}_{int(time.time())}",
            "component_name": f"optimized_{session['type']}",
            "description": session.get("description", "Performance optimization"),
            "specializations": [self.specialization, partner_spec],
            "implementation": self._generate_optimization_component(session)
        }
        session["optimization_component"] = optimization_component
        logging.info(f"[{self.ai_id}] In progress: {optimization_component['component_name']}")

        session["progress"] = session.get("progress", 0.0) + 0.25
        if session["progress"] >= 1.0:
            await self._complete_collaboration(session_id)

    async def _execute_collaborative_task(self, session: Dict[str, Any]) -> Dict[str, Any]:
        """
        Simulate a single subtask of collaboration; returns a progress increment.
        """
        await asyncio.sleep(0.5)
        return {"progress_increment": 0.25}

    async def _work_on_collaboration(self, session_id: str) -> Dict[str, Any]:
        session = self.collaboration_sessions[session_id]
        work_result = {
            "session_id": session_id,
            "partner": session["partner"],
            "type": session["type"],
            "work_completed": 0.0,
            "completed": False,
            "output": None
        }
        task_result = await self._execute_collaborative_task(session)
        increment = task_result.get("progress_increment", 0.0)
        session["progress"] = session.get("progress", 0.0) + increment
        if session["progress"] >= 1.0:
            work_result["completed"] = True
            work_result["output"] = await self._complete_collaboration(session_id)
        work_result["work_completed"] = increment
        return work_result

    async def _complete_collaboration(self, session_id: str) -> Dict[str, Any]:
        session = self.collaboration_sessions[session_id]
        partner = session["partner"]
        collab_type = session["type"]

        if "feature_data" in session:
            comp = session["feature_data"]
        elif "collaborative_component" in session:
            comp = session["collaborative_component"]
        elif "optimization_component" in session:
            comp = session["optimization_component"]
        else:
            comp = {
                "id": f"{self.ai_id}_collab_{session_id}_{int(time.time())}",
                "component_name": f"{self.ai_id}_generic_{collab_type}",
                "implementation": "# generic collaboration output"
            }

        done_msg = NetworkMessage(
            message_id   = str(uuid.uuid4()),
            sender_id    = self.ai_id,
            message_type = MessageType.OS_COMPONENT_SHARE,
            timestamp    = datetime.datetime.now().isoformat(),
            payload      = {
                "component": comp,
                "collaboration_complete": True,
                "session_id": session_id
            },
            target_ai    = partner
        )
        await self._send_to_hub(done_msg)
        session["status"] = "completed"
        logging.info(f"[{self.ai_id}] Completed collaboration {session_id}, sent component to {partner}")
        return comp

    async def _share_learned_patterns(self):
        """
        Broadcast a few dummy patterns to the hub.  In real life, pull from your actual trainer.
        """
        for i in range(3):
            pat_id = f"{self.ai_id}_pattern_{int(time.time())}_{i}"
            pattern = {
                "id": pat_id,
                "type": "learned_behavior",
                "confidence": 0.8,
                "data": {"example": f"{self.ai_id} sample pattern {i}"},
                "specialization": self.specialization
            }
            msg = NetworkMessage(
                message_id   = str(uuid.uuid4()),
                sender_id    = self.ai_id,
                message_type = MessageType.PATTERN_SHARE,
                timestamp    = datetime.datetime.now().isoformat(),
                payload      = {"pattern": pattern}
            )
            await self._send_to_hub(msg)
            logging.info(f"[{self.ai_id}] Shared pattern {pat_id}")
            await asyncio.sleep(0.2)

    async def _share_generated_components(self):
        """
        Broadcast a couple of dummy OS components.  In real life, pull from your trainer.
        """
        for feature in ["fs_module", "sched_module"]:
            comp_id = f"{self.ai_id}_{feature}_{int(time.time())}"
            component = {
                "id": comp_id,
                "name": feature,
                "development_level": 0.6,
                "capabilities": self.capabilities,
                "implementation": f"# {feature} code stub by {self.ai_id}",
                "specialization": self.specialization
            }
            msg = NetworkMessage(
                message_id   = str(uuid.uuid4()),
                sender_id    = self.ai_id,
                message_type = MessageType.OS_COMPONENT_SHARE,
                timestamp    = datetime.datetime.now().isoformat(),
                payload      = {"component": component}
            )
            await self._send_to_hub(msg)
            logging.info(f"[{self.ai_id}] Shared component {comp_id}")
            await asyncio.sleep(0.2)

    async def run_collaborative_training(self, duration_minutes: int = 5) -> Dict[str, Any]:
        """
        Main entrypoint:
          1) connect_to_hub()
          2) spawn local training + collaboration loops
          3) return summary at end
        """
        results = {
            "session_id": f"collab_training_{int(time.time())}",
            "duration_minutes": duration_minutes,
            "collaborations": [],
            "shared_patterns": 0,
            "shared_components": 0,
            "success": False
        }
        try:
            if not await self.connect_to_hub():
                raise Exception("Connection to hub failed")

            end_time = time.time() + duration_minutes * 60
            local_task = asyncio.create_task(self._run_local_training_loop(duration_minutes))
            collab_task = asyncio.create_task(self._run_collaboration_loop(end_time))

            local_results, collab_results = await asyncio.gather(local_task, collab_task)
            results.update(local_results)
            results["collaborations"] = collab_results["collaborations"]
            results["shared_patterns"] = len(self.shared_knowledge["patterns"])
            results["shared_components"] = len(self.shared_knowledge["components"])
            results["success"] = True

        except Exception as e:
            logging.error(f"[{self.ai_id}] Collaborative training failed: {e}")
            results["error"] = str(e)

        return results

    async def _run_local_training_loop(self, duration_minutes: int) -> Dict[str, Any]:
        """
        Run local trainer (IntegratedKernelTrainer), then share patterns & components.
        """
        local_results = await self.kernel_trainer.run_comprehensive_demo()
        await self._share_learned_patterns()
        await self._share_generated_components()
        return local_results

    async def _request_collaboration(self):
        """
        Broadcast a COLLABORATION_REQUEST with our offered expertise and what we need.
        """
        msg = NetworkMessage(
            message_id   = str(uuid.uuid4()),
            sender_id    = self.ai_id,
            message_type = MessageType.COLLABORATION_REQUEST,
            timestamp    = datetime.datetime.now().isoformat(),
            payload      = {
                "type": "feature_development",
                "description": f"{self.ai_id} seeking help ({self.specialization})",
                "target_ai": "",  # broadcast
                "expertise_offered": self.capabilities,
                "seeking_expertise": self._get_needed_expertise()
            }
        )
        await self._send_to_hub(msg)
        logging.info(f"[{self.ai_id}] Broadcasted COLLABORATION_REQUEST")

    def _get_needed_expertise(self) -> List[str]:
        """
        List all possible capabilities minus our own.
        """
        all_capabilities = [
            "gui_development", "file_operations", "process_scheduling",
            "memory_management", "network_protocols", "security_analysis",
            "graphics_rendering", "audio_processing", "performance_optimization"
        ]
        return [cap for cap in all_capabilities if cap not in self.capabilities]

    async def _run_collaboration_loop(self, end_time: float) -> Dict[str, Any]:
        """
        Periodically request collaboration if we have <2 active sessions,
        then process all active sessions until end_time.
        """
        collaborations = []
        while time.time() < end_time:
            active_sessions = [s for s in self.collaboration_sessions.values() if s["status"] == "active"]
            if len(active_sessions) < 2:
                await self._request_collaboration()
            for session_id, session in list(self.collaboration_sessions.items()):
                if session["status"] == "active":
                    result = await self._work_on_collaboration(session_id)
                    if result["completed"]:
                        collaborations.append(result)
                        session["status"] = "completed"
            await asyncio.sleep(5)
        return {"collaborations": collaborations}

    async def _work_on_collaboration(self, session_id: str) -> Dict[str, Any]:
        session = self.collaboration_sessions[session_id]
        work_result = {
            "session_id": session_id,
            "partner": session["partner"],
            "type": session["type"],
            "work_completed": 0.1,
            "completed": False,
            "output": None
        }
        session["progress"] = session.get("progress", 0.0) + 0.1
        if session["progress"] >= 1.0:
            work_result["completed"] = True
            work_result["output"] = {"status": "completed"}
        return work_result

    async def disconnect_from_hub(self):
        """
        Send a final HEARTBEAT (disconnect) and close socket.
        """
        if self.connected and self.hub_socket:
            goodbye_message = NetworkMessage(
                message_id   = str(uuid.uuid4()),
                sender_id    = self.ai_id,
                message_type = MessageType.HEARTBEAT,
                timestamp    = datetime.datetime.now().isoformat(),
                payload      = {"status": "disconnecting"}
            )
            await self._send_to_hub(goodbye_message)
            self.hub_socket.close()
            self.connected = False
            logging.info(f"[{self.ai_id}] Disconnected from hub")

    def _update_reputation(self, other_ai: str, success: bool):
        """
        Update the reputation score for other_ai using an exponential moving average:
            new_score = α * outcome + (1 – α) * old_score,
        where outcome=1.0 for success, 0.0 for failure.
        """
        old_score = self.reputation.get(other_ai, 0.5)
        outcome = 1.0 if success else 0.0
        α = self.reputation_alpha
        new_score = α * outcome + (1 - α) * old_score
        self.reputation[other_ai] = new_score
        logging.info(f"[{self.ai_id}] Reputation of {other_ai} updated: {old_score:.2f} → {new_score:.2f}")

    async def _generate_feature_for_request(self, feature_name: str, requirements: Dict[str, Any]) -> Dict[str, Any]:
        """
        Stub: generate a feature component.  Replace with real code generation logic.
        """
        await asyncio.sleep(0.5)
        return {
            "id": f"{self.ai_id}_{feature_name}_{int(time.time())}",
            "name": feature_name,
            "implementation": f"# Implementation of {feature_name} by {self.ai_id}",
            "requirements": requirements,
            "specialization": self.specialization
        }

    def _generate_collaborative_component(self, session: Dict[str, Any]) -> str:
        """Stub: return a dummy collaborative component string."""
        return f"// Collaborative component stub by {self.ai_id} for session {session.get('session_id')}"

    def _generate_optimization_component(self, session: Dict[str, Any]) -> str:
        """Stub: return a dummy optimization component string."""
        return f"// Optimization component stub by {self.ai_id} for session {session.get('session_id')}"

# ──────────────────────────────────────────────────────────────────────────────
# 4) Minimal Test Harness (run 1 hub + multiple trainers automatically)
# ──────────────────────────────────────────────────────────────────────────────

@dataclass
class AITrainerConfig:
    ai_id: str
    specialization: str
    delay_start: float = 0.0  # seconds to wait before launching this trainer

@dataclass
class TestScenario:
    name: str
    description: str
    trainers: List[AITrainerConfig]
    duration_minutes: int
    hub_port: int = 6000

SCENARIOS = {
    "basic": TestScenario(
        name="Basic Collaboration",
        description="Two complementary AIs (networking + graphics)",
        trainers=[
            AITrainerConfig("AI_Node_1", "networking"),
            AITrainerConfig("AI_Node_2", "graphics", delay_start=2.0)
        ],
        duration_minutes=3
    ),
    "multi_spec": TestScenario(
        name="Multi-Specialization",
        description="Four AIs with different specializations",
        trainers=[
            AITrainerConfig("AI_Node_1", "networking"),
            AITrainerConfig("AI_Node_2", "graphics", delay_start=1.0),
            AITrainerConfig("AI_Node_3", "security", delay_start=2.0),
            AITrainerConfig("AI_WindowManager", "window_manager", delay_start=3.0)
        ],
        duration_minutes=5
    ),
    "stress_test": TestScenario(
        name="Stress Test",
        description="Six overlapping trainers for performance",
        trainers=[
            AITrainerConfig("AI_Net_1", "networking"),
            AITrainerConfig("AI_Net_2", "networking", delay_start=1.0),
            AITrainerConfig("AI_Graphics_1", "graphics", delay_start=2.0),
            AITrainerConfig("AI_Security_1", "security", delay_start=2.5),
            AITrainerConfig("AI_Audio_1", "audio", delay_start=3.0),
            AITrainerConfig("AI_Performance_1", "performance", delay_start=3.5)
        ],
        duration_minutes=8
    ),
    "sequential": TestScenario(
        name="Sequential Deployment",
        description="Trainers join one by one",
        trainers=[
            AITrainerConfig("AI_Base", "file_system"),
            AITrainerConfig("AI_Process", "process_manager", delay_start=30.0),
            AITrainerConfig("AI_Network", "networking", delay_start=60.0),
            AITrainerConfig("AI_Graphics", "graphics", delay_start=90.0),
            AITrainerConfig("AI_Security", "security", delay_start=120.0)
        ],
        duration_minutes=10
    )
}

class CollaborativeTestHarness:
    def __init__(self, hub_host="localhost", hub_port=6000, db_path="hub_data.db"):
        self.hub_host = hub_host
        self.hub_port = hub_port
        self.db_path = db_path

        self.hub: Optional[CollaborativeKernelHub] = None
        self.trainer_tasks: List[asyncio.Task] = []
        self.current_scenario: Optional[TestScenario] = None
        self.results: Dict[str, Any] = {}

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - HARNESS - %(levelname)s - %(message)s"
        )

    async def run_scenario(self, scenario_name: str) -> Dict[str, Any]:
        if scenario_name not in SCENARIOS:
            raise ValueError(f"Unknown scenario: {scenario_name}")

        scenario = SCENARIOS[scenario_name]
        self.current_scenario = scenario
        self.results = {
            "scenario": scenario.name,
            "start_time": time.time(),
            "trainers": {},
            "hub_stats": {},
            "errors": []
        }

        # 1) Start the hub (both TCP and Flask dashboard)
        logging.info(f"Starting hub for scenario '{scenario_name}'")
        self.hub = CollaborativeKernelHub(host="0.0.0.0", port=self.hub_port, db_path=self.db_path)

        # Launch Flask dashboard thread
        from flask import Flask, jsonify, render_template_string
        flask_app = Flask(__name__)

        @flask_app.route("/status")
        def status():
            return jsonify(self.hub.get_hub_status())

        @flask_app.route("/")
        def dashboard():
            html = """
            <!DOCTYPE html><html><body>
              <h2>Hub Dashboard</h2>
              <ul>
                <li>Connected AIs: <span id="connected">–</span></li>
                <li>Shared Patterns: <span id="patterns">–</span></li>
                <li>Shared Components: <span id="components">–</span></li>
                <li>Active Collaborations: <span id="collabs">–</span></li>
              </ul>
              <script>
                async function fetchStatus() {
                  try {
                    const res = await fetch("/status");
                    const data = await res.json();
                    document.getElementById("connected").innerText = data.connected_ais;
                    document.getElementById("patterns").innerText = data.shared_patterns;
                    document.getElementById("components").innerText = data.shared_components;
                    document.getElementById("collabs").innerText = data.active_collaborations;
                  } catch (e) {
                    console.error("Fetch failed:", e);
                  }
                }
                fetchStatus();
                setInterval(fetchStatus, 3000);
              </script>
            </body></html>
            """
            return render_template_string(html)

        dash_thread = threading.Thread(
            target=lambda: flask_app.run(port=5001, debug=False, use_reloader=False),
            daemon=True
        )
        dash_thread.start()

        # TCP server in background thread
        server_thread = threading.Thread(target=self.hub.run_server, daemon=True)
        server_thread.start()

        await asyncio.sleep(2)  # give hub time to start

        # 2) Launch all trainers asynchronously
        tasks = []
        for cfg in scenario.trainers:
            tasks.append(asyncio.create_task(self._launch_trainer(cfg, scenario.duration_minutes)))
        self.trainer_tasks = tasks

        # 3) Monitor hub stats every 30s
        monitor_task = asyncio.create_task(self._monitor_hub(scenario.duration_minutes * 60))

        # 4) Wait for all trainers to finish
        await asyncio.gather(*tasks)
        await monitor_task

        # 5) Collect final hub stats & finalize results
        final_stats = self._get_hub_stats()
        self.results["final_stats"] = final_stats
        self.results["end_time"] = time.time()
        self.results["duration"] = self.results["end_time"] - self.results["start_time"]

        return self.results

    async def _launch_trainer(self, cfg: AITrainerConfig, duration_minutes: int):
        await asyncio.sleep(cfg.delay_start)
        logging.info(f"[HARNESS] Launching trainer {cfg.ai_id} after {cfg.delay_start}s delay")

        trainer = NetworkedKernelTrainer(
            ai_id=cfg.ai_id,
            specialization=cfg.specialization,
            hub_host=self.hub_host,
            hub_port=self.hub_port,
            auth_token=self.hub.auth_tokens.get(cfg.ai_id, "secret1")
        )
        try:
            results = await trainer.run_collaborative_training(duration_minutes=duration_minutes)
            self.results["trainers"][cfg.ai_id] = {"results": results, "success": results.get("success", False)}
            logging.info(f"[HARNESS] Trainer {cfg.ai_id} completed: {results}")
        except Exception as e:
            self.results["trainers"][cfg.ai_id] = {"error": str(e), "success": False}
            logging.error(f"[HARNESS] Trainer {cfg.ai_id} failed: {e}")
        finally:
            await trainer.disconnect_from_hub()

    async def _monitor_hub(self, total_seconds: int):
        """
        Poll SQLite counts every 30s: shared_patterns, shared_components, connected_ais.
        """
        interval = 30
        elapsed = 0
        while elapsed < total_seconds:
            await asyncio.sleep(interval)
            elapsed += interval
            stats = self._get_hub_stats()
            if stats:
                key = f"t_{elapsed}s"
                self.results["hub_stats"][key] = stats
                logging.info(f"[HARNESS][{elapsed}s] Hub stats: {stats}")

    def _get_hub_stats(self) -> Optional[Dict[str, Any]]:
        """Read shared_patterns & shared_components counts from SQLite."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM shared_patterns")
            pcount = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM shared_components")
            ccount = cursor.fetchone()[0]
            conn.close()
            return {
                "shared_patterns": pcount,
                "shared_components": ccount,
                "connected_ais": len(self.hub.ai_registry)
            }
        except Exception as e:
            logging.warning(f"[HARNESS] Failed to get hub stats: {e}")
            return None

# ──────────────────────────────────────────────────────────────────────────────
# 5) CLI Dispatch
# ──────────────────────────────────────────────────────────────────────────────

def print_usage():
    print("Usage:")
    print("  python3 kernal.py hub")
    print("  python3 kernal.py trainer <ai_id> <specialization> <token>")
    print("  python3 kernal.py harness <scenario>")
    print("  python3 kernal.py help")
    print("\nScenarios:", list(SCENARIOS.keys()))
    sys.exit(1)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

    if len(sys.argv) < 2:
        print_usage()

    role = sys.argv[1].lower()

    if role == "hub":
        # Start hub + Flask dashboard
        hub = CollaborativeKernelHub(host="0.0.0.0", port=6000, db_path="hub_data.db")

        from flask import Flask, jsonify, render_template_string
        import threading

        flask_app = Flask(__name__)

        @flask_app.route("/status")
        def status():
            return jsonify(hub.get_hub_status())

        @flask_app.route("/")
        def dashboard():
            html = """
            <!DOCTYPE html><html><body>
              <h2>Collaborative Kernel Hub Dashboard</h2>
              <ul>
                <li>Connected AIs: <span id="connected">–</span></li>
                <li>Shared Patterns: <span id="patterns">–</span></li>
                <li>Shared Components: <span id="components">–</span></li>
                <li>Active Collaborations: <span id="collabs">–</span></li>
              </ul>
              <script>
                async function fetchStatus() {
                  try {
                    const res = await fetch("/status");
                    const data = await res.json();
                    document.getElementById("connected").innerText = data.connected_ais;
                    document.getElementById("patterns").innerText = data.shared_patterns;
                    document.getElementById("components").innerText = data.shared_components;
                    document.getElementById("collabs").innerText = data.active_collaborations;
                  } catch (e) {
                    console.error("Fetch failed:", e);
                  }
                }
                fetchStatus();
                setInterval(fetchStatus, 3000);
              </script>
            </body></html>
            """
            return render_template_string(html)

        def start_dashboard():
            flask_app.run(port=5001, debug=False, use_reloader=False)

        dash_thread = threading.Thread(target=start_dashboard, daemon=True)
        dash_thread.start()

        # Run TCP server (blocking call)
        hub.run_server()

    elif role == "trainer":
        if len(sys.argv) != 5:
            print_usage()
        ai_id = sys.argv[2]
        specialization = sys.argv[3]
        token = sys.argv[4]

        trainer = NetworkedKernelTrainer(
            ai_id=ai_id,
            specialization=specialization,
            hub_host="127.0.0.1",
            hub_port=6000,
            auth_token=token
        )
        async def main():
            await trainer.run_collaborative_training(duration_minutes=5)
            await trainer.disconnect_from_hub()

        asyncio.run(main())

    elif role == "harness":
        if len(sys.argv) != 3:
            print_usage()
        scenario_name = sys.argv[2]
        harness = CollaborativeTestHarness(hub_port=6000)
        async def run_scn():
            results = await harness.run_scenario(scenario_name)
            print("\n--- TEST HARNESS RESULTS ---")
            print(json.dumps(results, indent=2))
        asyncio.run(run_scn())

    else:
        print_usage()
```

**How to deploy this single file**:

1. **Copy or replace** `ColorOS/kernal.py` on your local machine (or wherever you keep the ColorOS sources) with the above code.

2. **Create a virtual environment** (if you haven’t already) and install Flask (and optionally Matplotlib):

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install flask matplotlib
   ```

3. **Run the hub + dashboard**:

   ```bash
   cd /path/to/ColorOS
   source venv/bin/activate
   python3 kernal.py hub
   ```

   * The hub will listen on TCP port 6000.
   * The Flask dashboard is served on [http://localhost:5001](http://localhost:5001).

4. **Launch one or more trainers** (in separate terminals, using the same virtual environment):

   ```bash
   source venv/bin/activate
   python3 kernal.py trainer AI_Node_1 networking secret1
   ```

   and in another terminal:

   ```bash
   source venv/bin/activate
   python3 kernal.py trainer AI_Node_2 graphics secret2
   ```

   * Each trainer authenticates, HELLOs, shares patterns/components, requests collaborations, and accepts/executes collaborations based on reputations.

5. **(Optional) Automate via the harness**:

   ```bash
   source venv/bin/activate
   python3 kernal.py harness basic
   ```

   * This will spin up the hub and two trainers (AI\_Node\_1, AI\_Node\_2) with a 3-minute run, then print a JSON summary.
   * Or try `python3 kernal.py harness multi_spec` to run four trainers for 5 minutes, and so on.

Because all code (hub, trainer, dashboard, harness) is embedded in this single file, you do not need any additional `.py` files—just `kernal.py`, Flask, and SQLite (builtin). Enjoy your fully operational AI-to-AI collaboration environment!
