# ai_brain.py (MODIFIED for AI-Driven Code Generation)

import os
import google.generativeai as genai
from datetime import datetime
import time # Added for time.time()

# --- Configuration ---
try:
    API_KEY = os.environ.get("GOOGLE_API_KEY")
    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY environment variable not set.")
except Exception as e:
    print(f"Error loading API key: {e}")
    print("Please ensure GOOGLE_API_KEY is set as an environment variable.")
    API_KEY = None

if API_KEY:
    genai.configure(api_key=API_KEY)
    # Using a capable model for code generation
    model = genai.GenerativeModel('gemini-pro') # 'gemini-1.5-pro-latest' if available and needed
else:
    model = None

# Define directories for outputs
SCROLL_OUTPUT_DIR = "./scrolls"
ROADMAPS_OUTPUT_DIR = "./roadmaps"
GENERATED_CODE_DIR = "./generated_code" # NEW: Directory for AI-generated code files

os.makedirs(SCROLL_OUTPUT_DIR, exist_ok=True)
os.makedirs(ROADMAPS_OUTPUT_DIR, exist_ok=True)
os.makedirs(GENERATED_CODE_DIR, exist_ok=True) # Ensure generated_code directory exists


# --- Simple AI Query Function (now potentially uses Gemini) ---
def simple_ai_query(prompt: str) -> str:
    """
    A placeholder function that simulates an AI responding to a prompt.
    Now capable of generating simple .pxl scroll content or using Gemini if configured.
    """
    print(f"DEBUG: AI received prompt: '{prompt}'") # Debug print to console

    if model is None: # Fallback to rule-based if Gemini not initialized
        prompt_lower = prompt.lower()
        if "generate a simple scroll" in prompt_lower or "create a new pxl" in prompt_lower:
            new_scroll_id = f"AI_GENERATED_{int(time.time())}"
            ai_message = "Hello from an AI-generated scroll (offline mode)!"
            ai_color = "FF0000" # Red for offline
            response = f"""# PXLSCROLL_ID: {new_scroll_id}
# DESCRIPTION: An autonomous scroll generated by the AI (offline).
# TARGET_ZONE: MAIN_CONTENT

# COMMAND: CLEAR_ZONE
# ZONE_NAME: MAIN_CONTENT
# COLOR: 500000 # Dark red background
# COMMAND: WRITE_TEXT
# TEXT: {ai_message}
# X: 20
# Y: 20
# FONT_SIZE: 24
# COLOR: {ai_color}
# COMMAND: WRITE_TEXT
# TEXT: This is offline content.
# X: 25
# Y: 60
# FONT_SIZE: 16
# COLOR: FFFFFF
OUTPUT_TO_8PNG_ZONE: STATUS_BAR
OUTPUT_TEXT: "OFFLINE AI Scroll '{new_scroll_id}' Executed @ " + TIMESTAMP
OUTPUT_COLOR: FF0000
OUTPUT_TO_8PNG_ZONE: AI_FEEDBACK
OUTPUT_TEXT: "OFFLINE AI generated: {new_scroll_id}"
OUTPUT_COLOR: FF0000
"""
            print(f"DEBUG: Offline AI generated new scroll content for ID: {new_scroll_id}")
            return response
        else:
            # Existing rule-based responses
            if "hello" in prompt_lower or "hi" in prompt_lower:
                return "Greetings, Human! The pixel system is operational (offline)."
            elif "status" in prompt_lower or "how are you" in prompt_lower:
                return "All systems are nominal (offline). Ready for new instructions."
            else:
                return f"I'm a simple AI placeholder (offline). I process: '{prompt}'."
    
    # If Gemini is available, use it directly
    return gemini_ai_query(prompt)


# --- Gemini API call function (renamed for clarity, now main LLM call) ---
def gemini_ai_query(prompt_text):
    if model is None:
        print("ERROR: Gemini model not initialized. API key might be missing or invalid.")
        return "ERROR: Gemini model not available."

    try:
        response = model.generate_content(prompt_text)
        return response.text
    except Exception as e:
        print(f"ERROR: Failed to query Gemini API: {e}")
        return f"ERROR: AI API failed - {e}"

# --- Function to save the generated scroll to a file ---
def save_scroll_to_file(scroll_content: str, filename_prefix: str = "ai_generated_scroll"):
    """
    Saves the generated .pxl scroll content to a file in the SCROLL_OUTPUT_DIR.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{filename_prefix}_{timestamp}.pxl"
    filepath = os.path.join(SCROLL_OUTPUT_DIR, filename)

    try:
        with open(filepath, 'w') as f:
            f.write(scroll_content)
        print(f"INFO: Successfully saved AI-generated scroll to: {filepath}")
        return filepath
    except Exception as e:
        print(f"ERROR: Failed to save scroll to file {filepath}: {e}")
        return None

# Function to generate and save software roadmaps
def generate_software_roadmap(roadmap_request: str, roadmap_metadata: dict = None):
    """
    Generates a software roadmap using Google Gemini based on a request,
    and saves it to a Markdown file.
    """
    if model is None:
        return "ERROR: Gemini model not initialized. Cannot generate roadmap."

    roadmap_format_instructions = """
    Your output MUST be a software roadmap in Markdown format.
    Include sections for:
    - # Title
    - ## Overview
    - ## Phases
        - ### Phase 1: <Name>
            - Key Deliverables:
            - Estimated Timeline:
            - Dependencies:
        - ### Phase 2: <Name>
            - Key Deliverables:
            - Estimated Timeline:
            - Dependencies:
    - ## Key Considerations
    - ## Next Steps

    Be concise but comprehensive for a high-level plan.
    """

    prompt = f"""
    You are an AI assistant tasked with generating high-level software roadmaps.
    Your roadmap should address the following request:

    --- ROADMAP REQUEST ---
    {roadmap_request}

    --- METADATA ---
    """
    if roadmap_metadata:
        for key, value in roadmap_metadata.items():
            prompt += f"Metadata: {key}: {value}\n"
    else:
        prompt += f"Metadata: author:AI_Brain\n"
        prompt += f"Metadata: timestamp:{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\n"

    prompt += f"""
    --- ROADMAP FORMAT INSTRUCTIONS ---
    {roadmap_format_instructions}

    --- GENERATE ROADMAP HERE ---
    """

    print(f"\nDEBUG: Sending prompt to Gemini for roadmap generation...")
    generated_content = gemini_ai_query(prompt)

    if generated_content and not generated_content.startswith("ERROR:"):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"roadmap_{timestamp}.md"
        filepath = os.path.join(ROADMAPS_OUTPUT_DIR, filename)
        try:
            with open(filepath, 'w') as f:
                f.write(generated_content)
            print(f"INFO: Successfully saved AI-generated roadmap to: {filepath}")
            return filepath
        except Exception as e:
            print(f"ERROR: Failed to save roadmap to file {filepath}: {e}")
            return f"ERROR: Failed to save roadmap: {e}"
    else:
        print(f"Error generating roadmap content: {generated_content}")
        return f"ERROR: Failed to generate roadmap content: {generated_content}"


# NEW FUNCTION: To generate and save executable code
def generate_code(code_request: str, language: str = "python", filename_prefix: str = "ai_generated_code", code_metadata: dict = None):
    """
    Generates executable code using Google Gemini based on a request,
    and saves it to a file in the GENERATED_CODE_DIR.
    """
    if model is None:
        return "ERROR: Gemini model not initialized. Cannot generate code."

    # Adjust file extension based on language
    file_extension = {
        "python": "py",
        "javascript": "js",
        "html": "html",
        "css": "css",
        "json": "json",
        "markdown": "md"
    }.get(language.lower(), "txt")

    code_format_instructions = f"""
    Your output MUST be complete and runnable {language} code.
    Provide only the code, without extensive conversational preamble or postamble.
    Use comments within the code for explanations.
    If the request implies a file, provide the full file content.
    """

    prompt = f"""
    You are an AI developer assistant. Your task is to generate {language} code.
    The code should fulfill the following request:

    --- CODE GENERATION REQUEST ---
    {code_request}

    --- METADATA ---
    """
    if code_metadata:
        for key, value in code_metadata.items():
            prompt += f"Metadata: {key}: {value}\n"
    else:
        prompt += f"Metadata: author:AI_Brain_Dev\n"
        prompt += f"Metadata: timestamp:{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\n"

    prompt += f"""
    --- CODE FORMAT INSTRUCTIONS ---
    {code_format_instructions}

    --- GENERATE {language.upper()} CODE HERE ---
    """

    print(f"\nDEBUG: Sending prompt to Gemini for {language} code generation...")
    generated_content = gemini_ai_query(prompt)

    if generated_content and not generated_content.startswith("ERROR:"):
        # Extract code block if Gemini wraps it in markdown
        if f"```{language}" in generated_content and "```" in generated_content:
            start_idx = generated_content.find(f"```{language}") + len(f"```{language}")
            end_idx = generated_content.rfind("```")
            code_content = generated_content[start_idx:end_idx].strip()
        else:
            code_content = generated_content.strip() # Assume raw code

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.{file_extension}"
        filepath = os.path.join(GENERATED_CODE_DIR, filename)
        try:
            with open(filepath, 'w') as f:
                f.write(code_content)
            print(f"INFO: Successfully saved AI-generated code to: {filepath}")
            return filepath
        except Exception as e:
            print(f"ERROR: Failed to save code to file {filepath}: {e}")
            return f"ERROR: Failed to save code: {e}"
    else:
        print(f"Error generating code content: {generated_content}")
        return f"ERROR: Failed to generate code content: {generated_content}"


# --- Function to generate .pxl scrolls using Gemini (updated metadata and instructions) ---
def generate_pxl_scroll_with_gemini(user_request: str, scroll_metadata: dict = None):
    """
    Generates a .pxl scroll using Google Gemini based on a user request.
    """
    if model is None:
        return "ERROR: Gemini model not initialized. Cannot generate scroll."

    pxl_format_instructions = """
    Your output MUST be a .pxl scroll. Follow this exact format:

    # SCROLL_META: <key>:<value> #
    # SCROLL_META: <key>:<value> #
    # ...

    # PXL_COMMANDS #
    <command_line_1>
    <command_line_2>
    # ...

    Examples of commands:
    DEFINE_ZONE: <zone_name>, <x1>, <y1>, <x2>, <y2>
    SET_ZONE: <zone_name>
    DISPLAY_TEXT: <zone_name>, <text_string>
    SET_COLOR: <zone_name>, <R>, <G>, <B>
    WAIT: <seconds>
    # Special command for PixelNet hardware output:
    COMMAND: SEND_PIXELNET_FRAME
    DATA: <Hexadecimal_color_sequence_for_PixelNet_bus>
    # Example: DATA: FF0000FF0000 (Red, Green for two pixels)

    You can also use special placeholders in DISPLAY_TEXT commands that the system will replace with live data:
    {{TEMP}}   - Replaced by current simulated temperature (e.g., "22.5C")
    {{UPTIME}} - Replaced by system uptime (e.g., "1d 05h 30m")
    {{EVENTS}} - Replaced by a simple event counter
    {{TIME}}   - Replaced by current time (e.g., "14:30:00")
    {{RANDOM}} - Replaced by a random number (0-100)

    Ensure all lines start with a command or a hash (#) for comments/metadata.
    Be concise but provide clear instructions.
    The display resolution is 800x600 pixels (0,0 to 799,599).
    """

    prompt = f"""
    You are an AI assistant designed to write .pxl scrolls for a pixel display system.
    Your task is to generate a .pxl scroll based on the following user request.
    Adhere strictly to the .pxl scroll format provided below.

    --- USER REQUEST ---
    {user_request}

    --- SCROLL METADATA (OPTIONAL) ---
    """
    if scroll_metadata:
        for key, value in scroll_metadata.items():
            prompt += f"# SCROLL_META: {key}:{value}\n"
    else:
        prompt += f"# SCROLL_META: author:AI_Brain\n"
        prompt += f"# SCROLL_META: timestamp:{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\n"

    prompt += f"""
    --- PXL FORMAT INSTRUCTIONS ---
    {pxl_format_instructions}

    --- GENERATE SCROLL HERE ---
    """

    print(f"\nDEBUG: Sending prompt to Gemini for .pxl scroll generation...")
    generated_content = gemini_ai_query(prompt)
    return generated_content

# --- Testing the updated functionality ---
if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    logging.info("AI Brain starting up for standalone test...")
    
    # Ensure necessary directories for testing exist
    os.makedirs("./scrolls", exist_ok=True)
    os.makedirs("./roadmaps", exist_ok=True)
    os.makedirs("./generated_code", exist_ok=True)

    if model:
        logging.info("Testing scroll generation...")
        test_request_1 = "Generate a simple scroll that clears the MAIN_CONTENT zone to blue, then displays 'Hello AI World!' in white. Give it a unique PXLSCROLL_ID."
        filepath1 = generate_pxl_scroll_with_gemini(
            user_request=test_request_1, 
            scroll_metadata={"purpose": "standalone_test_scroll", "priority": "medium"}
        )
        if filepath1 and not filepath1.startswith("ERROR:"):
            save_scroll_to_file(filepath1, "standalone_ai_test_scroll")
        
        logging.info("Testing roadmap generation...")
        roadmap_request_1 = "Create a roadmap for a new module 'NetworkMonitor' including phases for design, implementation, and testing."
        filepath_roadmap = generate_software_roadmap(
            roadmap_request=roadmap_request_1,
            roadmap_metadata={"project": "NetworkMonitor_V1"}
        )
        logging.info(f"Roadmap generated result: {filepath_roadmap}")

        logging.info("Testing code generation...")
        code_request_1 = "Write a Python function `is_prime(number)` that returns true if a number is prime, false otherwise. Include a small unit test for it."
        filepath_code = generate_code(
            code_request=code_request_1,
            language="python",
            filename_prefix="prime_checker_test",
            code_metadata={"feature": "PrimeUtility", "tests_included": True}
        )
        logging.info(f"Code generated result: {filepath_code}")

    else:
        logging.warning("Gemini model not available - running offline mode tests.")
        # Test offline scroll generation if Gemini is not available
        offline_scroll_content = simple_ai_query("Generate a simple scroll for the system.")
        if offline_scroll_content and not offline_scroll_content.startswith("ERROR:"):
             save_scroll_to_file(offline_scroll_content, "offline_ai_test_scroll")
        
    logging.info("AI Brain standalone tests complete.")
